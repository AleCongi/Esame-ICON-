{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0a7HPIOuPjnesSEcfymu6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AleCongi/Esame-ICON-/blob/main/CNN_for_event_posters_and_OCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **I want to create a convolutional neural network that takes 2 types of images:poster of events , and images which are not poster of events ,  determines if  the image is indeed a poster and for those above a set level of  certanty then it   extracts the text inside it with OCR which will be later processed by other modules for NLP that will be able through mulitple source of knowledge aquired  predicts these features  reguarding the Event like :the name,the time and date, the place,the organizer,the type and general vibe,the latitude and longitude,the involvment of other entities like organizers or individuals,possibly the availability of tickets and tokens with the multiple ways to acquire them,the estimated number of partecipants and so on.. .**"
      ],
      "metadata": {
        "id": "RYnDz_O5iyHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I will use  a pre-trained model:VGG which is a model that has already been trained on a large dataset ,Imagenet in this case, and can be used as a starting point for solving a similar but different task. The idea behind using pre-trained models is that they have already learned useful features from the data they were trained on, which can be fine-tuned to the new task using a smaller dataset."
      ],
      "metadata": {
        "id": "-a0oWCoBnHyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras  import datasets,layers,models\n",
        "from keras.applications import VGG16\n",
        "from keras import losses\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Reshape"
      ],
      "metadata": {
        "id": "2XYSGzquGu0Q"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will advise you to use Google Drive or other alike serices to load the immages since colab has an annoying refresh rate every couple of hours which deletes every file present on your folder"
      ],
      "metadata": {
        "id": "harWV7E9nrEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "event_path = \"/content/drive/MyDrive/Colab Notebooks/yesPosters\"\n",
        "non_event_path = \"/content/drive/MyDrive/Colab Notebooks/noPosters\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQrhbSMH-wuJ",
        "outputId": "3d6b1e6a-f77d-4de7-e8b3-5070716e8a35"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I label each image with the correct class (event poster or non-event poster). This will allow the model to learn from the labeled data.\n",
        "\n",
        "I made sure in the Crawling phase  to balance the number of images for each class to feed this model, if there's a large imbalance in the dataset, the model might perform poorly.\n",
        "\n",
        "The image size is resized to (224, 224) pixels because it is a common size that is used in many pre-trained models such as VGG, ResNet, and Inception. These models were trained on a large dataset of images, and the images were resized to this size before being fed into the model.\n",
        "\n",
        "The reason why the image size of (224, 224) is so widely used in pre-trained models is because it is a good balance between the ability to capture fine-grained details in an image and computational efficiency.\n",
        "A larger image size would require more computation power and memory, which can be a bottleneck in some cases.\n",
        "\n",
        "Another reason is that many datasets used to train these models were collected and labeled with image size of (224, 224) pixels, so models trained on those datasets are optimized to work well with images of that size.\n",
        "In addition, this size is also compatible with most of the available hardware and software, which makes it more accessible for a wide range of users."
      ],
      "metadata": {
        "id": "tZXTA5CfoUxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize empty lists to store the images and labels\n",
        "images = []\n",
        "labels = []\n",
        "\n",
        "# Loop through the event posters and add them to the images and labels list\n",
        "for filename in os.listdir(event_path):\n",
        "    img = cv2.imread(os.path.join(event_path, filename))\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    images.append(img)\n",
        "    labels.append(1)\n",
        "\n",
        "# Loop through the non-event posters and add them to the images and labels list\n",
        "for filename in os.listdir(non_event_path):\n",
        "    img = cv2.imread(os.path.join(non_event_path, filename))\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "    images.append(img)\n",
        "    labels.append(0)\n"
      ],
      "metadata": {
        "id": "FSkZ_KlhAreC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An image is a 2-dimensional matrix of pixels. Each pixel in an image has a value that represents its color. The value of each pixel is usually represented by 3 values (R, G, B) which stands for Red, Green and Blue color channels respectively.\n",
        "\n",
        "When an image is loaded into python, it is usually represented as a numpy array, where each element of the array represents the value of a pixel. Each element of the array is a tuple of 3 values (R, G, B) which represent the color of the pixel.\n",
        "\n",
        "For example, an image of size (224, 224, 3) would be represented as a numpy array of shape (224, 224, 3) with each element of the array representing a pixel. The first two dimensions of the array represent the height and width of the image and the last dimension represents the color channel.\n",
        "\n",
        "So when using images = np.array(images), it converts the list of images, where each image is represented as a 2-dimensional matrix of pixels, into a 3-dimensional numpy array where each element of the array represents a pixel in an image.\n",
        "\n",
        "I'm using jpeg with RGB color space which  has 3 channels: Red, Green and Blue.\n",
        "\n",
        "The \"bits per color channel\" (bpc) is the number of bits that are used for storing each component 24 bit in my case , therefore each channel ranges from 0 to 256 so the last dim of the array for each three coordinates will go from zero to 256 max.\n",
        "\n",
        "It's worth noting that some image formats like grayscale images, have only one channel, in this case, the shape of the numpy array will be (224, 224) instead of (224, 224, 3)"
      ],
      "metadata": {
        "id": "nHxSjnSUqIyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the lists to numpy arrays\n",
        "images = np.array(images)\n",
        "labels = np.array(labels)\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "\n",
        "\n",
        "# Split the data into training and test sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42,stratify=labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoYE0M9LB99n",
        "outputId": "0cbf9afb-e09f-450e-b78a-f54a6564bc7c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2036, 224, 224, 3)\n",
            "(2036,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qc4MrKBfjYM",
        "outputId": "cce9bb30-da87-4f41-c101-e27127bf861f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1628, 224, 224, 3)\n",
            "(1628,)\n",
            "(408, 224, 224, 3)\n",
            "(408,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\n",
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNRzbR2tGuaZ",
        "outputId": "244f108b-63ff-437e-ab46-b0a381099247"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (1628, 224, 224, 3)\n",
            "1628 train samples\n",
            "408 test samples\n",
            "x_train shape: (1628, 224, 224, 3)\n",
            "1628 train samples\n",
            "408 test samples\n",
            "y_train shape: (1628,)\n",
            "y_test shape: (408,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        "model = VGG16(weights='imagenet', include_top=False,input_shape=(224, 224, 3))\n",
        "from keras.models import Model\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag9cUuOzCQ_x",
        "outputId": "d13d41f1-b4eb-4520-8691-2408dd7edd55"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 128\n",
        "num_classes = 2\n",
        "epochs = 12\n"
      ],
      "metadata": {
        "id": "SjPkLDagFYiB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_XMRNGXetEr",
        "outputId": "2fe70cd4-b643-4b46-f63c-f92cddf328c8"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[ 24  91 154]\n",
            "  [ 21  97 159]\n",
            "  [ 27 107 168]\n",
            "  ...\n",
            "  [ 32  61 146]\n",
            "  [ 30  59 144]\n",
            "  [ 24  53 138]]\n",
            "\n",
            " [[ 37 104 167]\n",
            "  [ 36 112 174]\n",
            "  [ 31 111 172]\n",
            "  ...\n",
            "  [ 35  68 152]\n",
            "  [ 35  67 152]\n",
            "  [ 28  60 145]]\n",
            "\n",
            " [[ 33 106 164]\n",
            "  [ 27 105 162]\n",
            "  [ 32 113 170]\n",
            "  ...\n",
            "  [ 23  59 147]\n",
            "  [ 27  61 149]\n",
            "  [ 34  66 152]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 26  22  98]\n",
            "  [ 26  22  98]\n",
            "  [ 26  22  98]\n",
            "  ...\n",
            "  [ 26  22  98]\n",
            "  [ 26  22  98]\n",
            "  [ 26  22  98]]\n",
            "\n",
            " [[ 26  22  98]\n",
            "  [ 26  22  98]\n",
            "  [ 26  22  98]\n",
            "  ...\n",
            "  [ 26  22  98]\n",
            "  [ 26  22  98]\n",
            "  [ 26  22  98]]\n",
            "\n",
            " [[ 26  22  98]\n",
            "  [ 26  22  98]\n",
            "  [ 26  22  98]\n",
            "  ...\n",
            "  [ 26  22  98]\n",
            "  [ 26  22  98]\n",
            "  [ 26  22  98]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Freezing the layers of a pre-trained model, except the final fully connected layer, is done to prevent the model from forgetting the features it has already learned. The pre-trained model has already learned useful features from the data it was trained on, and it's important to keep these learned features while fine-tuning the model on a new dataset.\n",
        "The VGG16 model that you used is pre-trained on the ImageNet dataset and is designed to recognize 1000 different classes of objects. The final layer of the model is a fully connected layer with 1000 units, each corresponding to one of the 1000 classes.\n",
        "\n",
        "In my task, i want to use the VGG16 model to predict if an image is an event poster or not, which is a binary classification task. So i need to replace the final fully connected layer of the VGG16 model with a new fully connected layer that has only 2 units, one for each class (event poster and non-event poster).\n",
        "\n",
        "I removed the last fully connected layer by using model.layers.pop() and then added a new fully connected layer with only 2 units using the Dense(1, activation='sigmoid') layer. This new layer is connected to the output of the VGG16 model, and its output will be used as the final prediction for the task.\n",
        "\n",
        "I also added in between a Flatten level since the fully connected layers in a neural network expect their input to be a one-dimensional array, so the Flatten() layer is used to convert the 3D tensor( The output of the VGG16 model is a 3D tensor of shape (batch_size, height, width, channels)) into a 1D array, which can then be used as input to the fully connected layers.\n",
        "\n",
        "I added finally a new dense layer with 256 units and relu activation function, this dense layer is added to increase the representational power of the model,it's called bottleneck layer. It's used to prevent overfitting and increase the ability of the model to generalize."
      ],
      "metadata": {
        "id": "9_sBr2VGyFs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the last fully connected layer\n",
        "model.layers.pop()\n",
        "\n",
        "# Freeze all the layers\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add a new fully connected layer\n",
        "x = model.output\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=model.input, outputs=x)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "2ChS-4lFZ9E0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train)\n",
        "print(y_train)\n",
        "y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
        "y_test = np.asarray(y_test).astype('float32').reshape((-1,1))\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LECaNiW1Cdsg",
        "outputId": "30f5054c-a604-40fc-dad3-69729ccaa7de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[[ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   ...\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]]\n",
            "\n",
            "  [[ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   ...\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]]\n",
            "\n",
            "  [[ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   ...\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 63  63  63]\n",
            "   [ 63  63  63]\n",
            "   [ 61  61  61]\n",
            "   ...\n",
            "   [ 63  63  63]\n",
            "   [ 63  63  63]\n",
            "   [ 63  63  63]]\n",
            "\n",
            "  [[ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   [ 63  63  63]\n",
            "   ...\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]]\n",
            "\n",
            "  [[ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   ...\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]\n",
            "   [ 64  64  64]]]\n",
            "\n",
            "\n",
            " [[[  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   ...\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]]\n",
            "\n",
            "  [[  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   ...\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]]\n",
            "\n",
            "  [[  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   ...\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   ...\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]]\n",
            "\n",
            "  [[  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   ...\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]]\n",
            "\n",
            "  [[  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   ...\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]]]\n",
            "\n",
            "\n",
            " [[[  0   0   0]\n",
            "   [  0   0   0]\n",
            "   [  0   0   0]\n",
            "   ...\n",
            "   [  0   0   0]\n",
            "   [  0   0   0]\n",
            "   [  0   0   0]]\n",
            "\n",
            "  [[  0   0   0]\n",
            "   [  0   0   0]\n",
            "   [  0   0   0]\n",
            "   ...\n",
            "   [  0   0   0]\n",
            "   [  0   0   0]\n",
            "   [  0   0   0]]\n",
            "\n",
            "  [[  0   0   0]\n",
            "   [  0   0   0]\n",
            "   [  0   0   0]\n",
            "   ...\n",
            "   [  0   0   0]\n",
            "   [  0   0   0]\n",
            "   [  0   0   0]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   ...\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]]\n",
            "\n",
            "  [[  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   ...\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]]\n",
            "\n",
            "  [[  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   ...\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]\n",
            "   [  1   1   1]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[ 21 161 227]\n",
            "   [ 20 160 226]\n",
            "   [ 20 161 227]\n",
            "   ...\n",
            "   [ 17 162 230]\n",
            "   [ 17 162 230]\n",
            "   [ 17 162 230]]\n",
            "\n",
            "  [[ 21 161 227]\n",
            "   [ 20 160 226]\n",
            "   [ 20 160 227]\n",
            "   ...\n",
            "   [ 17 162 230]\n",
            "   [ 17 162 230]\n",
            "   [ 17 162 230]]\n",
            "\n",
            "  [[ 20 159 228]\n",
            "   [ 22 159 228]\n",
            "   [ 22 159 229]\n",
            "   ...\n",
            "   [ 17 162 230]\n",
            "   [ 19 162 230]\n",
            "   [ 18 161 229]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 13  42 203]\n",
            "   [ 13  42 203]\n",
            "   [ 14  42 203]\n",
            "   ...\n",
            "   [204 156 113]\n",
            "   [202 152 110]\n",
            "   [203 158 115]]\n",
            "\n",
            "  [[ 13  42 203]\n",
            "   [ 13  42 203]\n",
            "   [ 14  42 203]\n",
            "   ...\n",
            "   [205 158 117]\n",
            "   [207 162 119]\n",
            "   [207 161 118]]\n",
            "\n",
            "  [[ 13  41 202]\n",
            "   [ 14  42 203]\n",
            "   [ 13  41 202]\n",
            "   ...\n",
            "   [202 155 114]\n",
            "   [205 160 117]\n",
            "   [206 158 118]]]\n",
            "\n",
            "\n",
            " [[[253 252 255]\n",
            "   [253 252 255]\n",
            "   [253 252 255]\n",
            "   ...\n",
            "   [228 220 227]\n",
            "   [228 220 227]\n",
            "   [228 220 227]]\n",
            "\n",
            "  [[253 252 255]\n",
            "   [253 252 255]\n",
            "   [253 252 255]\n",
            "   ...\n",
            "   [228 220 227]\n",
            "   [228 220 227]\n",
            "   [228 220 227]]\n",
            "\n",
            "  [[253 252 254]\n",
            "   [253 252 254]\n",
            "   [253 252 254]\n",
            "   ...\n",
            "   [228 220 227]\n",
            "   [228 220 227]\n",
            "   [228 220 227]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[254 254 254]\n",
            "   [254 254 254]\n",
            "   [254 254 254]\n",
            "   ...\n",
            "   [207 200 203]\n",
            "   [207 200 203]\n",
            "   [206 199 202]]\n",
            "\n",
            "  [[254 254 254]\n",
            "   [254 254 254]\n",
            "   [254 254 254]\n",
            "   ...\n",
            "   [207 200 203]\n",
            "   [206 199 202]\n",
            "   [206 199 202]]\n",
            "\n",
            "  [[254 254 254]\n",
            "   [254 254 254]\n",
            "   [254 254 254]\n",
            "   ...\n",
            "   [207 200 203]\n",
            "   [206 199 202]\n",
            "   [206 199 202]]]\n",
            "\n",
            "\n",
            " [[[ 30 198   7]\n",
            "   [ 30 198   7]\n",
            "   [ 30 198   7]\n",
            "   ...\n",
            "   [ 30 198   7]\n",
            "   [ 30 198   7]\n",
            "   [ 30 198   7]]\n",
            "\n",
            "  [[ 30 198   7]\n",
            "   [ 30 198   7]\n",
            "   [ 30 198   7]\n",
            "   ...\n",
            "   [ 30 198   7]\n",
            "   [ 30 198   7]\n",
            "   [ 30 198   7]]\n",
            "\n",
            "  [[ 30 198   7]\n",
            "   [ 30 198   7]\n",
            "   [ 30 198   7]\n",
            "   ...\n",
            "   [ 30 198   7]\n",
            "   [ 30 198   7]\n",
            "   [ 30 198   7]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[201  53 225]\n",
            "   [201  53 225]\n",
            "   [201  53 225]\n",
            "   ...\n",
            "   [ 48 232 248]\n",
            "   [ 48 232 248]\n",
            "   [ 48 232 248]]\n",
            "\n",
            "  [[201  53 225]\n",
            "   [201  53 225]\n",
            "   [201  53 225]\n",
            "   ...\n",
            "   [ 48 232 248]\n",
            "   [ 48 232 248]\n",
            "   [ 48 232 248]]\n",
            "\n",
            "  [[201  53 225]\n",
            "   [201  53 225]\n",
            "   [201  53 225]\n",
            "   ...\n",
            "   [ 48 232 248]\n",
            "   [ 48 232 248]\n",
            "   [ 48 232 248]]]]\n",
            "[[1.]\n",
            " [0.]\n",
            " [0.]\n",
            " ...\n",
            " [1.]\n",
            " [0.]\n",
            " [1.]]\n",
            "Epoch 1/12\n",
            "13/13 [==============================] - 1125s 88s/step - loss: 0.6903 - accuracy: 0.5399 - val_loss: 0.6836 - val_accuracy: 0.5686\n",
            "Epoch 2/12\n",
            "13/13 [==============================] - 1138s 89s/step - loss: 0.6815 - accuracy: 0.5915 - val_loss: 0.6781 - val_accuracy: 0.5735\n",
            "Epoch 3/12\n",
            "13/13 [==============================] - 1092s 85s/step - loss: 0.6777 - accuracy: 0.5934 - val_loss: 0.6736 - val_accuracy: 0.5907\n",
            "Epoch 4/12\n",
            "13/13 [==============================] - 1092s 85s/step - loss: 0.6746 - accuracy: 0.5934 - val_loss: 0.6691 - val_accuracy: 0.5833\n",
            "Epoch 5/12\n",
            "13/13 [==============================] - 1110s 87s/step - loss: 0.6720 - accuracy: 0.5934 - val_loss: 0.6657 - val_accuracy: 0.5882\n",
            "Epoch 6/12\n",
            "13/13 [==============================] - 1124s 88s/step - loss: 0.6697 - accuracy: 0.5921 - val_loss: 0.6638 - val_accuracy: 0.5858\n",
            "Epoch 7/12\n",
            "13/13 [==============================] - 1104s 87s/step - loss: 0.6673 - accuracy: 0.5946 - val_loss: 0.6620 - val_accuracy: 0.5784\n",
            "Epoch 8/12\n",
            "13/13 [==============================] - 1052s 82s/step - loss: 0.6667 - accuracy: 0.5897 - val_loss: 0.6607 - val_accuracy: 0.5711\n",
            "Epoch 9/12\n",
            "13/13 [==============================] - 1049s 82s/step - loss: 0.6657 - accuracy: 0.5946 - val_loss: 0.6605 - val_accuracy: 0.5833\n",
            "Epoch 10/12\n",
            "13/13 [==============================] - 1041s 81s/step - loss: 0.6651 - accuracy: 0.5915 - val_loss: 0.6599 - val_accuracy: 0.5760\n",
            "Epoch 11/12\n",
            " 1/13 [=>............................] - ETA: 12:58 - loss: 0.7281 - accuracy: 0.4688"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "W-iBSE3xiwn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "id": "zjpznB65FHM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a model can take a significant amount of time, especially when working with large datasets or complex models. Saving the trained model allows you to use it later without having to retrain it.\n",
        "You can save the architecture of the model using the model.to_json() method, and save the weights using the model.save_weights() method. Later, you can load the architecture and weights separately using the model_from_json() and model.load_weights() methods.\n",
        "You can save the entire model like im doing, including the architecture, weights, and optimizer state, using the model.save() method. Later, you can load the entire model using the keras.models.load_model() method."
      ],
      "metadata": {
        "id": "DtLvXcvEwmjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('eventPosterRecognizer.h5')"
      ],
      "metadata": {
        "id": "KOuaaxwHwZoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now i will discuss the Optical Character Recognition(OCR) part, which i will be  using to extract as much semantic as possible from the image."
      ],
      "metadata": {
        "id": "Eli3-RlD1FyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VdmtIqY71KYN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python-tesseract is a wrapper for Google’s Tesseract-OCR Engine. It is also useful as a stand-alone invocation script to tesseract, as it can read all image types supported by the Pillow and Leptonica imaging libraries, including jpeg, png, gif, bmp, tiff, and others. Additionally, if used as a script, Python-tesseract will print the recognized text instead of writing it to a file."
      ],
      "metadata": {
        "id": "NQmeeRcn23iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-hj4K-rLjLv",
        "outputId": "9014df81-d4bc-47ca-eced-a00ec654f34c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.8/dist-packages (0.3.10)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.8/dist-packages (from pytesseract) (9.4.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.8/dist-packages (from pytesseract) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=21.3->pytesseract) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from pytesseract import pytesseract\n",
        "import shutil\n",
        "import os\n",
        "import cv2\n",
        "import json\n",
        "import random\n",
        "try:\n",
        " from PIL import Image\n",
        "except ImportError:\n",
        " import Image"
      ],
      "metadata": {
        "id": "ntEa71mGLX3U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By the way you will need to also have the binary associated with it ,download it from the site and remember the PATH where you have stored it."
      ],
      "metadata": {
        "id": "s8FABWiL3AmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbToDXoxNT3h",
        "outputId": "76bf8e12-c618-4345-c83f-98fb9656ffdd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 21 not upgraded.\n",
            "Need to get 4,795 kB of archives.\n",
            "After this operation, 15.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-eng all 4.00~git24-0e00fe6-1.2 [1,588 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr-osd all 4.00~git24-0e00fe6-1.2 [2,989 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tesseract-ocr amd64 4.00~git2288-10f4998a-2 [218 kB]\n",
            "Fetched 4,795 kB in 2s (2,695 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 124016 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_4.00~git24-0e00fe6-1.2_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.00~git2288-10f4998a-2_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Setting up tesseract-ocr-osd (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr-eng (4.00~git24-0e00fe6-1.2) ...\n",
            "Setting up tesseract-ocr (4.00~git2288-10f4998a-2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the model we have previously saved and the images\n",
        "and resize the image, my model was trained on 224x224X3 pictures so before making predictions you need to convert these images too."
      ],
      "metadata": {
        "id": "v0VRNryr334_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = keras.models.load_model('eventPosterRecognizer.h5')\n",
        "new_images= \"/content/drive/MyDrive/Colab Notebooks/newImages\"\n",
        "# Resize the new images to (224, 224)\n",
        "new_images = [cv2.resize(image, (224, 224)) for image in new_images]"
      ],
      "metadata": {
        "id": "_tPbdJlaN1cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The predict() function will return an array of probabilities for each class, I will  choose a threshold to decide if the image is an event poster or not and save the result in a json file with a parameter for the image path."
      ],
      "metadata": {
        "id": "9b5c1r2V5Aea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = loaded_model.predict(new_images)\n",
        "treshold=0.8\n",
        "# Initialize a list to store the extracted text and image information\n",
        "text_list = []"
      ],
      "metadata": {
        "id": "hva3vRCu5BIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the image is first converted to grayscale using the cv2.cvtColor() function. Then, Otsu thresholding is applied to the grayscale image using the cv2.threshold() function to improve the quality of the text recognition. Finally, the image is passed to the pytesseract.image_to_string() function with the lang='ita+eng' parameter to specify that the text is in Italian or English. This will return the recognized text as a string."
      ],
      "metadata": {
        "id": "50amP7Zq9A79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(image):\n",
        "    # Convert the image to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Apply Otsu thresholding\n",
        "    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    # Pass the image to tesseract\n",
        "    text = pytesseract.image_to_string(thresh, lang='ita+eng')\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "G299xfgE8ore"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop through the predictions and extract text from the images with a prediction greater than the threshold\n",
        "for i, image in enumerate(new_images):\n",
        "    if predictions[i][1] > threshold:\n",
        "        # Extract text from the image\n",
        "        text = extract_text(image)\n",
        "        # Append the text and image information to the list\n",
        "        text_list.append({\"text\": text, \"image_path\": image_paths[i]})\n",
        "\n",
        "# Save the list as a json file\n",
        "with open('text_data.json', 'w') as f:\n",
        "    json.dump(text_list, f)\n",
        "        \n"
      ],
      "metadata": {
        "id": "FHGHGaAr5cFw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}